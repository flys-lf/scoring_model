{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL SELECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from mlflow.models import infer_signature\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score,ConfusionMatrixDisplay, recall_score, f1_score, roc_curve, roc_auc_score, fbeta_score, make_scorer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RepeatedStratifiedKFold\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from preprocessing import preprocessing\n",
    "import re\n",
    "import statsmodels.api as sm\n",
    "from sklearn.pipeline import Pipeline\n",
    "import mlflow\n",
    "from mlflow.models import infer_signature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(356251, 42)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/df_final.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(307507, 42)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df['TARGET'].notnull()]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df['TARGET']\n",
    "df_train = df.drop(labels='TARGET', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((307507, 41), (307507,))"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape, Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Train Data : Train, Test, Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = [f for f in df_train.columns if f not in ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index']]\n",
    "df_train[feats].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((215254, 40), (92253, 40))"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_train[feats], Y, stratify=Y, test_size=0.3, random_state=101)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TARGET\n",
       "0.0    91.927212\n",
       "1.0     8.072788\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()/len(y_train) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler() # Distribution des données n'est pas normale\n",
    "scaler.fit(df_train[feats])\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonction de coût métier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objectif :** Prédiction sur la probabilité de faillite d'un client (classe positive de la variable Cible), le client aura des difficultés de paiement.\n",
    "Minimiser les Faux Négatifs (FN), et maximiser la précision de la prédiction d'un client en défaut (1) qui est la classe minoritaire.\n",
    "Maximiser le rappel pour minimiser les Faux Négatifs (Prédiction d'absence de diffcultés de paiement pour un client qui a en réalité des difficultés de paiement) => Erreur de type II.\n",
    "\n",
    "On souhaite éviter en priorité d'accorder un prêt à un mauvais client à tord => Coût FN > Coût FP\n",
    "\n",
    "Suivi du score **F2 ou Fbeta**, on supposera que le coût d'un FN est **10 foix supérieur** que le coût d'un FP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate f2-measure\n",
    "def f2_measure(y_true, y_pred):\n",
    "    return fbeta_score(y_true, y_pred, beta=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate a model\n",
    "def evaluate_model(X, y, model):\n",
    "    # define evaluation procedure\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=101)\n",
    "    # define the model evaluation metric\n",
    "    metric = make_scorer(f2_measure)\n",
    "    # evaluate model\n",
    "    scores = cross_val_score(model, X, y, scoring=metric, cv=cv, n_jobs=-2)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_model_mlflow(x_train, metric_name, metric, model, model_name):\n",
    "    with mlflow.start_run():\n",
    "        params = {\n",
    "            \"nthread\":6,\n",
    "            \"n_estimators\":10000,\n",
    "            \"learning_rate\":0.02,\n",
    "            \"num_leaves\":34,\n",
    "            \"colsample_bytree\":0.9497036,\n",
    "            \"subsample\":0.8715623,\n",
    "            \"max_depth\":8,\n",
    "            \"reg_alpha\":0.041545473,\n",
    "            \"reg_lambda\":0.0735294,\n",
    "            \"min_split_gain\":0.0222415,\n",
    "            \"min_child_weight\":39.3259775,\n",
    "            \"silent\":-1,\n",
    "            \"verbose\":-1\n",
    "        }\n",
    "        mlflow.log_params(params)\n",
    "\n",
    "        # Log the loss metric\n",
    "        mlflow.log_metric(metric_name, metric)\n",
    "        # mlflow.log_metric(\"f1_score\", metric) # à renommer roc_auc\n",
    "\n",
    "        # Set a tag that we can use to remind ourselves what this run was for\n",
    "        mlflow.set_tag(\"Training Info\", model_name)\n",
    "        # Infer the model signature\n",
    "        signature = infer_signature(x_train, model.predict(x_train))\n",
    "        # Log the model\n",
    "        model_info = mlflow.sklearn.log_model(\n",
    "            sk_model=model,\n",
    "            artifact_path=model_name,\n",
    "            signature=signature,\n",
    "            registered_model_name=model_name,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_metrics(model, y_test, y_pred, y_prob):\n",
    "    metrics = {\n",
    "        \"f2_score\": fbeta_score(y_test, y_pred, beta=10),\n",
    "        \"accuracy_score\": accuracy_score(y_test, y_pred), # Utilisation de accuracy_score\n",
    "        \"score\": model.score(X_test, y_test), # Ou utilisation de la méthode score\n",
    "        \"recall\": recall_score(y_true=y_test, y_pred=y_pred),        \n",
    "        \"f1_score\": f1_score(y_true=y_test, y_pred=y_pred),\n",
    "        \"auc\": roc_auc_score(y_test, y_prob),    \n",
    "    }\n",
    "\n",
    "    print(f\"La méthode accuracy_score donne: {metrics['accuracy_score']}\")\n",
    "    print(f\"La méthode score donne: {metrics['score']}\")\n",
    "    print(f\"Le recall est de: {metrics['recall']}\")\n",
    "    print(f\"Le F1-score est de: {metrics['f1_score']}\")\n",
    "    print(f\"L'AUC est de: {metrics['auc']}\")\n",
    "    print(f\"Le F2-score est de: {metrics['f2_score']}\")\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = ensemble.HistGradientBoostingRegressor()\n",
    "params = {'learning_rate': [1e-1, 2e-1, 3e-1],\n",
    "          'max_depth': [2, 4, 5],\n",
    "          'min_samples_leaf': [30, 31, 32],\n",
    "          'max_iter': [100, 150]\n",
    "          }\n",
    "gsv = model_selection.GridSearchCV(regressor, params, cv=5)\n",
    "gsv.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Model Dummy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Métriques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the reference model\n",
    "model = DummyClassifier(strategy='most_frequent')\n",
    "# evaluate the model\n",
    "scores = evaluate_model(X_train_scaled, y_train, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calcul du F2 pour le Modèle de Référence strategy most_frequent et sans variable explicative :\n",
      "Score F2 : moyenne = 0.00%, écart_type = 0.00%\n"
     ]
    }
   ],
   "source": [
    "# summarize performance\n",
    "print(\"Calcul du F2 pour le Modèle de Référence strategy most_frequent et sans variable explicative :\")\n",
    "print(f\"Score F2 : moyenne = {np.mean(scores):.2%}, écart_type = {np.std(scores):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.0)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "# models.append((\"LogisticRegression\",LogisticRegression()))\n",
    "models.append((\"LinearSVC\",LinearSVC()))\n",
    "models.append((\"DecisionTree\",DecisionTreeClassifier()))\n",
    "models.append((\"RandomForest\",RandomForestClassifier()))\n",
    "models.append((\"XGBClassifier\",XGBClassifier()))\n",
    "models.append((\"LGBM_Classifier\",LGBMClassifier()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "    result = cross_val_score(model, X_train_scaled, y_train,  cv=3)\n",
    "    names.append(name)\n",
    "    results.append(result)\n",
    "\n",
    "for i in range(len(names)):\n",
    "    print(names[i],results[i].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model Dummy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La méthode accuracy_score donne: 0.9192654981409819\n",
      "La méthode score donne: 0.9192654981409819\n",
      "Le recall est de: 0.0\n",
      "Le F1-score est de: 0.0\n",
      "L'AUC est de: 0.5\n",
      "Le F2-score est de: 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'f2_score': np.float64(0.0),\n",
       " 'accuracy_score': 0.9192654981409819,\n",
       " 'score': 0.9192654981409819,\n",
       " 'recall': np.float64(0.0),\n",
       " 'f1_score': np.float64(0.0),\n",
       " 'auc': np.float64(0.5)}"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dummy uniform strategy = uniform/ most_frequent / stratified\n",
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\").fit(X_train_scaled, y_train)\n",
    "y_pred = dummy_clf.predict(X_test_scaled)\n",
    "y_train_pred = dummy_clf.predict(X_train_scaled)\n",
    "\n",
    "# Prédire les probabilités\n",
    "y_prob = dummy_clf.predict_proba(X_test_scaled)[:, 1]\n",
    "y_train_prob = dummy_clf.predict_proba(X_train_scaled)[:, 1]\n",
    "metrics = get_model_metrics(dummy_clf, y_test, y_pred, y_prob)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'DummyClassifier' already exists. Creating a new version of this model...\n",
      "2024/11/29 00:40:45 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: DummyClassifier, version 3\n",
      "Created version '3' of model 'DummyClassifier'.\n",
      "2024/11/29 00:40:45 INFO mlflow.tracking._tracking_service.client: 🏃 View run omniscient-roo-216 at: http://localhost:8080/#/experiments/672589219446151488/runs/6da0e5a50c8a49e6a7d70c1d9f1dbfe8.\n",
      "2024/11/29 00:40:45 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://localhost:8080/#/experiments/672589219446151488.\n"
     ]
    }
   ],
   "source": [
    "# Set MLflow Experiment ---------------------------------------------------------\n",
    "mlflow.set_tracking_uri(uri=\"http://localhost:8080\")\n",
    "mlflow.set_experiment(\"credit_score_experiment\")\n",
    "log_model_mlflow(X_train_scaled, \"f2_score\", metrics['f2_score'], dummy_clf, \"DummyClassifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fly\\Documents\\Data_Disk_D\\Data\\fly\\Mes documents\\FTTH Data\\formation_oc\\scoring_model\\.venv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\fly\\Documents\\Data_Disk_D\\Data\\fly\\Mes documents\\FTTH Data\\formation_oc\\scoring_model\\.venv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=0).fit(X_train_scaled, y_train)\n",
    "y_pred__lr = clf.predict(X_test)\n",
    "y_train_pred__lr = clf.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = LinearSVC().fit(X_train_scaled, y_train)\n",
    "y_pred__svc = svc.predict(X_test_scaled)\n",
    "y_train_pred__svc = svc.predict(X_train_scaled)\n",
    "\n",
    "# Prédire les probabilités\n",
    "y_prob__svc = svc._predict_proba_lr(X_test_scaled)[:, 1]\n",
    "y_train_prob__svc = svc._predict_proba_lr(X_train_scaled)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La méthode accuracy_score donne: 0.9192546583850931\n",
      "La méthode score donne: 0.5394079325333594\n",
      "Le recall est de: 0.0\n",
      "Le F1-score est de: 0.0\n",
      "L'AUC est de: 0.7248651927265247\n",
      "Le F2-score est de: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fly\\Documents\\Data_Disk_D\\Data\\fly\\Mes documents\\FTTH Data\\formation_oc\\scoring_model\\.venv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but LinearSVC was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'f2_score': np.float64(0.0),\n",
       " 'accuracy_score': 0.9192546583850931,\n",
       " 'score': 0.5394079325333594,\n",
       " 'recall': np.float64(0.0),\n",
       " 'f1_score': np.float64(0.0),\n",
       " 'auc': np.float64(0.7248651927265247)}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_SVC = get_model_metrics(svc, y_test, y_pred__svc, y_prob__svc)\n",
    "metrics_SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'LinearSVC'.\n",
      "2024/11/29 00:49:16 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: LinearSVC, version 1\n",
      "Created version '1' of model 'LinearSVC'.\n",
      "2024/11/29 00:49:16 INFO mlflow.tracking._tracking_service.client: 🏃 View run bedecked-stork-715 at: http://localhost:8080/#/experiments/672589219446151488/runs/8322d377cda643789463d56c9b3b24bb.\n",
      "2024/11/29 00:49:16 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://localhost:8080/#/experiments/672589219446151488.\n"
     ]
    }
   ],
   "source": [
    "log_model_mlflow(X_train_scaled, \"f2_score\", metrics_SVC['f2_score'], svc, \"LinearSVC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((215254, 40), (215254,))"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fly\\Documents\\Data_Disk_D\\Data\\fly\\Mes documents\\FTTH Data\\formation_oc\\scoring_model\\.venv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La méthode accuracy_score donne: 0.919330536676314\n",
      "La méthode score donne: 0.16770186335403728\n",
      "Le recall est de: 0.002551020408163265\n",
      "Le F1-score est de: 0.0050802139037433155\n",
      "L'AUC est de: 0.7207379113428285\n",
      "Le F2-score est de: 0.0025764199175116\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(random_state=0).fit(X_train_scaled, y_train)\n",
    "y_pred__rfc = rfc.predict(X_test_scaled)\n",
    "# y_train_pred__rfc = rfc.predict(X_train_scaled)\n",
    "\n",
    "# Prédire les probabilités\n",
    "y_prob__rfc = rfc.predict_proba(X_test_scaled)[:, 1]\n",
    "# y_train_prob__rfc = rfc.predict_proba(X_train_scaled)[:, 1]\n",
    "metrics_RFC = get_model_metrics(rfc, y_test, y_pred__rfc, y_prob__rfc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'RandomForestClassifier'.\n",
      "2024/11/29 01:35:16 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: RandomForestClassifier, version 1\n",
      "Created version '1' of model 'RandomForestClassifier'.\n",
      "2024/11/29 01:35:16 INFO mlflow.tracking._tracking_service.client: 🏃 View run omniscient-stag-410 at: http://localhost:8080/#/experiments/672589219446151488/runs/ed0b32c7e91249188ac07f19c67fd2f4.\n",
      "2024/11/29 01:35:16 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://localhost:8080/#/experiments/672589219446151488.\n"
     ]
    }
   ],
   "source": [
    "log_model_mlflow(X_train_scaled, \"f2_score\", metrics_RFC['f2_score'], rfc, \"RandomForestClassifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92253"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_pred__rfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fly\\Documents\\Data_Disk_D\\Data\\fly\\Mes documents\\FTTH Data\\formation_oc\\scoring_model\\.venv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La méthode accuracy_score donne: 0.9192654981409819\n",
      "La méthode score donne: 0.9192654981409819\n",
      "Le recall est de: 0.005773361976369495\n",
      "Le F1-score est de: 0.011414919033713831\n",
      "L'AUC est de: 0.7315477090267931\n",
      "Le F2-score est de: 0.0058304223733564595\n"
     ]
    }
   ],
   "source": [
    "rfc_balanced = RandomForestClassifier(random_state=0, class_weight=\"balanced\").fit(X_train_scaled, y_train)\n",
    "y_pred__rfc_balanced = rfc_balanced.predict(X_test_scaled)\n",
    "# y_train_pred__rfc = rfc.predict(X_train_scaled)\n",
    "\n",
    "# Prédire les probabilités\n",
    "y_prob__rfc_balanced = rfc_balanced.predict_proba(X_test_scaled)[:, 1]\n",
    "# y_train_prob__rfc = rfc.predict_proba(X_train_scaled)[:, 1]\n",
    "metrics_RFC_Balanced = get_model_metrics(rfc_balanced, y_test, y_pred__rfc_balanced, y_prob__rfc_balanced)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'RandomForestClassifier' already exists. Creating a new version of this model...\n",
      "2024/11/29 12:54:31 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: RandomForestClassifier, version 2\n",
      "Created version '2' of model 'RandomForestClassifier'.\n",
      "2024/11/29 12:54:31 INFO mlflow.tracking._tracking_service.client: 🏃 View run crawling-calf-746 at: http://localhost:8080/#/experiments/672589219446151488/runs/0423f3b662bd4273adbcf37c265601e3.\n",
      "2024/11/29 12:54:31 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://localhost:8080/#/experiments/672589219446151488.\n"
     ]
    }
   ],
   "source": [
    "log_model_mlflow(X_train_scaled, \"f2_score\", metrics_RFC_Balanced['f2_score'], rfc_balanced, \"RandomForestClassifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LGBM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 17377, number of negative: 197877\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.057043 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9711\n",
      "[LightGBM] [Info] Number of data points in the train set: 215254, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "La méthode accuracy_score donne: 0.7191202454120733\n",
      "La méthode score donne: 0.7527560079347013\n",
      "Le recall est de: 0.6821965628356605\n",
      "Le F1-score est de: 0.28169873038753673\n",
      "L'AUC est de: 0.7315477090267931\n",
      "Le F2-score est de: 0.6635166130954998\n"
     ]
    }
   ],
   "source": [
    "clf = LGBMClassifier(objective= 'binary', class_weight=\"balanced\").fit(X_train_scaled, y_train)\n",
    "y_pred__clf = clf.predict(X_test_scaled)\n",
    "y_prob__clf = rfc_balanced.predict_proba(X_test_scaled)\n",
    "metrics_LGBM = get_model_metrics(clf, y_test, y_pred__clf, y_prob__clf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob__clf = rfc_balanced.predict_proba(X_test_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'LGBMClassifier' already exists. Creating a new version of this model...\n",
      "2024/11/29 14:59:38 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: LGBMClassifier, version 2\n",
      "Created version '2' of model 'LGBMClassifier'.\n",
      "2024/11/29 14:59:38 INFO mlflow.tracking._tracking_service.client: 🏃 View run peaceful-bear-131 at: http://localhost:8080/#/experiments/672589219446151488/runs/d0508890b35c445491f65478b927b493.\n",
      "2024/11/29 14:59:38 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://localhost:8080/#/experiments/672589219446151488.\n"
     ]
    }
   ],
   "source": [
    "log_model_mlflow(X_train_scaled, \"f2_score\", metrics_LGBM['f2_score'], clf, \"LGBMClassifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model File -- Pickling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score : 71.91 %\n",
      "F2 score : 66.35 %\n",
      "[0. 1. 0. ... 0. 0. 1.]\n",
      "[[0.59252341 0.40747659]\n",
      " [0.48312982 0.51687018]\n",
      " [0.75870168 0.24129832]\n",
      " ...\n",
      " [0.82041659 0.17958341]\n",
      " [0.9094301  0.0905699 ]\n",
      " [0.1289471  0.8710529 ]]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "# Save to file in the current working directory\n",
    "pkl_filename = \"model.pkl\"\n",
    "with open(pkl_filename, 'wb') as file:\n",
    "    pickle.dump(clf, file)\n",
    "\n",
    "# Load from file\n",
    "with open(pkl_filename, 'rb') as file:\n",
    "    pickle_model = pickle.load(file)\n",
    "    \n",
    "# Calculate the accuracy score and predict target values\n",
    "f2_score = metrics_LGBM['f2_score']\n",
    "score = pickle_model.score(X_test_scaled, y_test)\n",
    "print(\"score : {0:.2f} %\".format(100 * score))\n",
    "print(\"F2 score : {0:.2f} %\".format(100 * f2_score))\n",
    "\n",
    "Y_pred = pickle_model.predict(X_test_scaled)\n",
    "Y_proba = pickle_model.predict_proba(X_test_scaled)\n",
    "print(Y_pred)\n",
    "print(Y_proba)\n",
    "\n",
    "y_pred_df = pd.DataFrame(Y_pred, columns=['y_pred_test'])\n",
    "y_pred_proba_df = pd.DataFrame(Y_proba, columns=['proba_classe_0', 'proba_classe_1'])\n",
    "\n",
    "# Récupération du score du client\n",
    "y_pred_proba_df = pd.DataFrame(y_pred_proba_df, columns=['proba_classe_0', 'proba_classe_1'])\n",
    "y_pred_proba_df = pd.concat([y_pred_proba_df['proba_classe_1'], X_test['SK_ID_CURR']], axis=1)\n",
    "# score = y_pred_df[y_pred_df['SK_ID_CURR']==ID_client]\n",
    "# score_value = round(score.proba_classe_1.iloc[0]*100, 2)\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# score = pickle_model.score(x_test, y_test)\n",
    "# print(\"Test score: {0:.2f} %\".format(100 * score))\n",
    "# Ypredict = pickle_model.predict(x_test)\n",
    "\n",
    "# ##loading the model from the saved file\n",
    "# pkl_filename = \"model.pkl\"\n",
    "# with open(pkl_filename, 'rb') as f_in:\n",
    "#     model = pickle.load(f_in)\n",
    "\n",
    "# predictValue = predict_mpg(config, model)\n",
    "# predictValue\n",
    "\n",
    "\n",
    "# y_pred_lgbm = model_LGBM.predict(lecture_X_test_clean().drop(labels=\"sk_id_curr\", axis=1))    # Prédiction de la classe 0 ou 1\n",
    "# y_pred_lgbm_proba = model_LGBM.predict_proba(lecture_X_test_clean().drop(labels=\"sk_id_curr\", axis=1)) # Prédiction du % de risque\n",
    "\n",
    "# # Récupération du score du client\n",
    "# y_pred_lgbm_proba_df = pd.DataFrame(y_pred_lgbm_proba, columns=['proba_classe_0', 'proba_classe_1'])\n",
    "# y_pred_lgbm_proba_df = pd.concat([y_pred_lgbm_proba_df['proba_classe_1'],\n",
    "#                                 lecture_X_test_clean()['sk_id_curr']], axis=1)\n",
    "# #st.dataframe(y_pred_lgbm_proba_df)\n",
    "# score = y_pred_lgbm_proba_df[y_pred_lgbm_proba_df['sk_id_curr']==ID_client]\n",
    "# score_value = round(score.proba_classe_1.iloc[0]*100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>proba_classe_0</th>\n",
       "      <th>proba_classe_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.592523</td>\n",
       "      <td>0.407477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.483130</td>\n",
       "      <td>0.516870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.758702</td>\n",
       "      <td>0.241298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.407403</td>\n",
       "      <td>0.592597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.782169</td>\n",
       "      <td>0.217831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92248</th>\n",
       "      <td>0.945559</td>\n",
       "      <td>0.054441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92249</th>\n",
       "      <td>0.280925</td>\n",
       "      <td>0.719075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92250</th>\n",
       "      <td>0.820417</td>\n",
       "      <td>0.179583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92251</th>\n",
       "      <td>0.909430</td>\n",
       "      <td>0.090570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92252</th>\n",
       "      <td>0.128947</td>\n",
       "      <td>0.871053</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>92253 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       proba_classe_0  proba_classe_1\n",
       "0            0.592523        0.407477\n",
       "1            0.483130        0.516870\n",
       "2            0.758702        0.241298\n",
       "3            0.407403        0.592597\n",
       "4            0.782169        0.217831\n",
       "...               ...             ...\n",
       "92248        0.945559        0.054441\n",
       "92249        0.280925        0.719075\n",
       "92250        0.820417        0.179583\n",
       "92251        0.909430        0.090570\n",
       "92252        0.128947        0.871053\n",
       "\n",
       "[92253 rows x 2 columns]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_proba_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DF PROCESSED MANUALLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(307511, 167)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('input/df_final_manual_feature_engineering.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df['TARGET']\n",
    "df.drop(labels='TARGET', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df, Y, stratify=Y, test_size=0.3, random_state=101)\n",
    "X_train.shape, X_test.shape\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(df.drop(labels=['SK_ID_CURR'], axis=1))\n",
    "X_train_scaled = scaler.transform(X_train.drop(labels='SK_ID_CURR', axis=1))\n",
    "X_test_scaled = scaler.transform(X_test.drop(labels='SK_ID_CURR', axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define models to test\n",
    "def get_models():\n",
    "    models, names = list(), list()\n",
    "    # LR\n",
    "    models.append(LogisticRegression())\n",
    "    names.append('Logistic_Regression')\n",
    "    # SVM\n",
    "    models.append(LinearSVC())\n",
    "    names.append('Linear_SVC')\n",
    "    # Random Forest\n",
    "    models.append(RandomForestClassifier())\n",
    "    names.append('Random_Forest_Classifier')\n",
    "    # XGB\n",
    "    models.append(XGBClassifier())\n",
    "    names.append('XGB_Classifier') \n",
    "    # LGBM\n",
    "    models.append(LGBMClassifier())\n",
    "    names.append('LGBM_Classifier')\n",
    "    return models, names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score F2 du modèle Logistic_Regression : moyenne = 1.80%, écart_type = 0.32%\n",
      "Temps de Calcul pour Logistic_Regression :          200.99 secondes\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[122], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# wrap the model in a pipeline\u001b[39;00m\n\u001b[0;32m     11\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m Pipeline(steps\u001b[38;5;241m=\u001b[39m[(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;124m'\u001b[39m, models[i])])\n\u001b[1;32m---> 12\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m results\u001b[38;5;241m.\u001b[39mappend(scores)\n\u001b[0;32m     14\u001b[0m duration\u001b[38;5;241m=\u001b[39mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;241m-\u001b[39mtemps\n",
      "Cell \u001b[1;32mIn[38], line 8\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(X, y, model)\u001b[0m\n\u001b[0;32m      6\u001b[0m metric \u001b[38;5;241m=\u001b[39m make_scorer(f2_measure)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# evaluate model\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mcross_val_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m scores\n",
      "File \u001b[1;32mc:\\Users\\fly\\Documents\\Data_Disk_D\\Data\\fly\\Mes documents\\FTTH Data\\formation_oc\\scoring_model\\.venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\fly\\Documents\\Data_Disk_D\\Data\\fly\\Mes documents\\FTTH Data\\formation_oc\\scoring_model\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:712\u001b[0m, in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[0;32m    710\u001b[0m scorer \u001b[38;5;241m=\u001b[39m check_scoring(estimator, scoring\u001b[38;5;241m=\u001b[39mscoring)\n\u001b[1;32m--> 712\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    715\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    723\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    724\u001b[0m \u001b[43m    \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    725\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\fly\\Documents\\Data_Disk_D\\Data\\fly\\Mes documents\\FTTH Data\\formation_oc\\scoring_model\\.venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\fly\\Documents\\Data_Disk_D\\Data\\fly\\Mes documents\\FTTH Data\\formation_oc\\scoring_model\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:423\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[0;32m    421\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[0;32m    422\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[1;32m--> 423\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    424\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    425\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscorer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscorers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscore_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscorer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    435\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    436\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_times\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    437\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_estimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_estimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    438\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    439\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    440\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\n\u001b[0;32m    441\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    443\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[0;32m    445\u001b[0m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\fly\\Documents\\Data_Disk_D\\Data\\fly\\Mes documents\\FTTH Data\\formation_oc\\scoring_model\\.venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     73\u001b[0m )\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\fly\\Documents\\Data_Disk_D\\Data\\fly\\Mes documents\\FTTH Data\\formation_oc\\scoring_model\\.venv\\Lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\fly\\Documents\\Data_Disk_D\\Data\\fly\\Mes documents\\FTTH Data\\formation_oc\\scoring_model\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\fly\\Documents\\Data_Disk_D\\Data\\fly\\Mes documents\\FTTH Data\\formation_oc\\scoring_model\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# define models\n",
    "models, names = get_models()\n",
    "results = list()\n",
    "\n",
    "# evaluate each model\n",
    "for i in range(len(models)):\n",
    "    temps = time.time()\n",
    "    # wrap the model in a pipeline\n",
    "    pipeline = Pipeline(steps=[('m', models[i])])\n",
    "    scores = evaluate_model(X_train_scaled, y_train, pipeline)\n",
    "    results.append(scores)\n",
    "    duration=time.time()-temps\n",
    "    # summarize and store\n",
    "    print(f\"Score F2 du modèle {names[i]} : moyenne = {np.mean(scores):.2%}, écart_type = {np.std(scores):.2%}\")\n",
    "    print(f\"Temps de Calcul pour {names[i]} : {duration:>15.2f} secondes\")\n",
    "    print(end='\\n\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
